{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib tk\n",
    "import os\n",
    "import skimage\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATASET\n",
    "filename1 = \"E:\\\\ts2010a\\\\image_labels_level_0.txt\"\n",
    "data = []\n",
    "f = open(filename1, \"r\")\n",
    "data = f.readlines()\n",
    "f.close()\n",
    "data\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace(\"\\t\", \" \")\n",
    "#     data[i] = data[i].replace(\"\", \" \")\n",
    "    data[i] = data[i].replace(\"\\n\", \"\")\n",
    "    data[i] = str.split(data[i],' ')\n",
    "sorted_data = [value for index, value in sorted(enumerate(data), key=lambda data:data[1])]\n",
    "cols = [\"image_id\", \"split_id\", \"shape_id\"]\n",
    "df_image_0 = pd.DataFrame(sorted_data, columns = cols)\n",
    "df_image_0.head(15)\n",
    "\n",
    "filename2=\"E:\\\\ts2010a\\\\image_labels_level_1.txt\"\n",
    "data = []\n",
    "f = open(filename2, \"r\")\n",
    "data = f.readlines()\n",
    "f.close()\n",
    "data\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace(\"\\t\", \" \")\n",
    "#     data[i] = data[i].replace(\"\", \" \")\n",
    "    data[i] = data[i].replace(\"\\n\", \"\")\n",
    "    data[i] = str.split(data[i],' ')\n",
    "sorted_data = [value for index, value in sorted(enumerate(data), key=lambda data:data[1])]\n",
    "cols = [\"image_id\", \"split_id\", \"sign_id\"]\n",
    "df_image_1 = pd.DataFrame(sorted_data, columns = cols)\n",
    "df_image_1.head(15)\n",
    "\n",
    "filename3 = \"E:\\\\ts2010a\\\\object_labels_level_0.txt\"\n",
    "data = []\n",
    "f = open(filename3, \"r\")\n",
    "data = f.readlines()\n",
    "f.close()\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace(\"\\t\", \" \")\n",
    "    data[i] = data[i].replace(\",\", \" \")\n",
    "    data[i] = data[i].replace(\"\\n\", \"\")\n",
    "    data[i] = str.split(data[i],' ')\n",
    "sorted_data = [value for index, value in sorted(enumerate(data), key=lambda data:data[1])]\n",
    "cols = [\"image_id\", \"shape_id\", \"x\", \"y\", \"height\", \"width\"]\n",
    "df_object_0 = pd.DataFrame(sorted_data, columns = cols)\n",
    "\n",
    "filename4 = \"E:\\\\ts2010a\\\\object_labels_level_1.txt\"\n",
    "data = []\n",
    "f = open(filename4, \"r\")\n",
    "data = f.readlines()\n",
    "f.close()\n",
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace(\"\\t\", \" \")\n",
    "    data[i] = data[i].replace(\",\", \" \")\n",
    "    data[i] = data[i].replace(\"\\n\", \"\")\n",
    "    data[i] = str.split(data[i],' ')\n",
    "sorted_data = [value for index, value in sorted(enumerate(data), key=lambda data:data[1])]\n",
    "cols = [\"image_id\", \"sign_id\", \"x\", \"y\", \"height\", \"width\"]\n",
    "df_object_1 = pd.DataFrame(sorted_data, columns = cols)\n",
    "\n",
    "df_image = pd.merge(df_image_0, df_image_1, how = \"inner\", on = ['image_id', 'split_id'])\n",
    "df_image.head(5)\n",
    "\n",
    "df_object = pd.merge(df_object_0, df_object_1, how = \"inner\", on = ['image_id', 'x', 'y', 'height', 'width'])\n",
    "order = [\"image_id\", \"shape_id\", \"sign_id\", \"x\", \"y\", \"height\", \"width\"]\n",
    "df_object = df_object[order]\n",
    "df_object.head(15)\n",
    "\n",
    "filenames = []\n",
    "for filename in os.listdir(\"E:\\\\ts2010a\\\\images\"):\n",
    "    filenames.append(filename)\n",
    "filenames.sort()\n",
    "a=len(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING\n",
    "length = len(filenames)\n",
    "train_objects = []\n",
    "train_shape = []\n",
    "train_sign = []\n",
    "test_objects = []\n",
    "test_shape = []\n",
    "test_sign = []\n",
    "count = 0\n",
    "# m = 0\n",
    "# ob = 0\n",
    "# im = 0\n",
    "for i in range(length):\n",
    "    s = \"E:\\\\ts2010a\\\\images\\\\\" + filenames[i]\n",
    "    img = Image.open(s, \"r\")#.convert('L')\n",
    "    img_name = df_image[\"image_id\"][i]\n",
    "    if len(df_object[df_object[\"image_id\"] == img_name]) == 0:\n",
    "           continue\n",
    "    index = df_object[df_object[\"image_id\"] == img_name].index[0]\n",
    "#     index = df_object[df_object[\"image_id\"] == img_name].index[0]\n",
    "    number = df_image.iat[i,2].count(',') + 1    # count the number of objects contained in one image\n",
    "    if df_image.iat[i,1] == \"1\":      # split trainset and testset, \"1\" for training, \"0\" for testing\n",
    "        for j in range(number):\n",
    "            # crop image\n",
    "            a = int(df_object[\"x\"][index+j])\n",
    "            b = int(df_object[\"y\"][index+j])\n",
    "            c = int(df_object[\"height\"][index+j]) + a\n",
    "            d = int(df_object[\"width\"][index+j]) + b\n",
    "            CropImg = img.crop((a,b,c,d))\n",
    "            CropImg = CropImg.resize((32,32))\n",
    "            enh_con = ImageEnhance.Contrast(CropImg) \n",
    "            contrast = 2.5\n",
    "            img_contrasted = enh_con.enhance(contrast)\n",
    "            CropImg = np.array(img_contrasted)\n",
    "#             if max(CropImg.shape) > m:\n",
    "#                 m = max(CropImg.shape)\n",
    "#                 im = i\n",
    "#                 ob = count\n",
    "#             CropImg = CropImg.reshape(3072)\n",
    "            train_objects.append(CropImg)                   # add image into trainset\n",
    "            train_shape.append(df_object.iat[index+j,1])      # add its shape id\n",
    "            train_sign.append(df_object.iat[index+j,2])       # add its sign id\n",
    "            \n",
    "            \n",
    "    elif df_image.iat[i,1] == \"0\":      # process testset\n",
    "        for j in range(number):\n",
    "            # crop image\n",
    "            a = int(df_object[\"x\"][index+j])\n",
    "            b = int(df_object[\"y\"][index+j])\n",
    "            c = int(df_object[\"height\"][index+j]) + a\n",
    "            d = int(df_object[\"width\"][index+j]) + b\n",
    "            CropImg = img.crop((a,b,c,d))\n",
    "            CropImg = CropImg.resize((32,32))\n",
    "            enh_con = ImageEnhance.Contrast(CropImg) \n",
    "            contrast = 2.5\n",
    "            img_contrasted = enh_con.enhance(contrast)\n",
    "            CropImg = np.array(img_contrasted)\n",
    "#             CropImg = CropImg.reshape(3072)\n",
    "#             if max(CropImg.shape) > m:\n",
    "#                 m = max(CropImg.shape)\n",
    "#                 im = i\n",
    "#                 ob = count\n",
    "            test_objects.append(CropImg)                   # add image into testset\n",
    "            test_shape.append(df_object.iat[index+j,1])      # add its shape id\n",
    "            test_sign.append(df_object.iat[index+j,2])       # add its sign id\n",
    "train_objects = np.array(train_objects)\n",
    "train_shape = np.array(train_shape)\n",
    "train_sign = np.array(train_sign)\n",
    "test_objects = np.array(test_objects)\n",
    "test_shape = np.array(test_shape)\n",
    "test_sign = np.array(test_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "for i in range(train_objects.shape[0]):\n",
    "    temp_object = train_objects[i,:,:,:]\n",
    "    mean = np.mean(temp_object)\n",
    "    std = np.max([np.std(temp_object), 1.0 / np.sqrt(train_objects.shape[1] * train_objects.shape[2] * train_objects.shape[3])])\n",
    "    train_objects[i,:,:,:] = (temp_object - mean) / std\n",
    "    \n",
    "for i in range(test_objects.shape[0]):\n",
    "    temp_object = test_objects[i,:,:,:]\n",
    "    mean = np.mean(temp_object)\n",
    "    std = np.max([np.std(temp_object), 1.0 / np.sqrt(test_objects.shape[1] * test_objects.shape[2] * test_objects.shape[3])])\n",
    "    test_objects[i,:,:,:] = (temp_object - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESIZE & RENAME LABELS\n",
    "for a in range(len(train_sign)):\n",
    "    train_shape[a]=int(train_shape[a])-1\n",
    "\n",
    "        \n",
    "for a in range(len(test_sign)):\n",
    "    test_shape[a]=int(test_shape[a])-1\n",
    "\n",
    "\n",
    "images_train=[skimage.transform.resize(image, (24, 24)) for image in np.array(train_objects)]\n",
    "train_shape=np.array(train_shape)\n",
    "train_sign = np.array(train_sign)\n",
    "test_shape = np.array(test_shape)\n",
    "test_sign = np.array(test_sign)\n",
    "images_test = [skimage.transform.resize(image, (24, 24)) for image in np.array(test_objects)]\n",
    "images_train = np.array(images_train)\n",
    "images_test = np.array(images_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTEATE VALIDATION DATASET\n",
    "import random\n",
    "random_number = random.sample(range(len(images_test)),700)\n",
    "validation_objects = []\n",
    "validation_sign = []\n",
    "for i in random_number:\n",
    "    validation_objects.append(images_test[i])\n",
    "    validation_sign.append(test_sign[i])\n",
    "validation_objects = np.array(validation_objects)\n",
    "validation_sign = np.array(validation_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "def weight_varible(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding = 'SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides=[1,2,2,1],padding = 'SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    xs=tf.placeholder(tf.float32,[None,24,24,3], name='x')\n",
    "    ys=tf.placeholder(tf.float32,[None,88], name='y')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    keep_hide = tf.placeholder(tf.float32, name='kee_hide')\n",
    "# lr1 = tf.Variable(0.0001, dtype=tf.float32)  \n",
    "global_step = tf.Variable(0, dtype=tf.int32)\n",
    "# lr2 = tf.Variable(0.00001, dtype=tf.float32)\n",
    "# lr3 = tf.Variable(0.000005, dtype=tf.float32)  \n",
    "lr = tf.cond(tf.less(global_step, 200), \n",
    "                     lambda: tf.constant(0.001),\n",
    "                     lambda: tf.constant(0.0001))\n",
    "# learing_rate = tf.train.exponential_decay(0.001,120,10,0.96,staircase=False)\n",
    "x_image = tf.reshape(xs,[-1,24,24,3])\n",
    "keep_conv = 0.8\n",
    "keep_hiden = 0.5\n",
    "\n",
    "# #conv0\n",
    "# W_conv0 = weight_varible([5,5,3,16])\n",
    "# b_conv0 = bias_variable([16])\n",
    "# h_conv0 = tf.nn.relu(conv2d(x_image,W_conv0)+b_conv0)\n",
    "# fc_mean,fc_var = tf.nn.moments(h_conv0,axes = [0])\n",
    "# scale = tf.Variable(tf.ones([16]))\n",
    "# shift = tf.Variable(tf.zeros([16]))\n",
    "# # h_LRN_conv1 = tf.nn.local_response_normalization(h_conv1,1,0,1,1)\n",
    "# # h_bn_conv1 = tf.nn.batch_normalization(h_conv1,fc_mean,fc_var,shift,scale,0.001)\n",
    "# h_pool0 = max_pool_2x2(h_conv0)\n",
    "# h_dropout0 = tf.nn.dropout(h_pool0, keep_conv)\n",
    "\n",
    "# conv1 layer\n",
    "W_conv1 = weight_varible([5,5,3,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "fc_mean,fc_var = tf.nn.moments(h_conv1,axes = [0])\n",
    "scale = tf.Variable(tf.ones([32]))\n",
    "shift = tf.Variable(tf.zeros([32]))\n",
    "# h_LRN_conv1 = tf.nn.local_response_normalization(h_conv1,1,0,1,1)\n",
    "h_bn_conv1 = tf.nn.batch_normalization(h_conv1,fc_mean,fc_var,shift,scale,0.001)\n",
    "h_pool1 = max_pool_2x2(h_bn_conv1)\n",
    "h_dropout1 = tf.nn.dropout(h_pool1, keep_conv)\n",
    "\n",
    "# conv2 layer\n",
    "W_conv2 = weight_varible([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_dropout1,W_conv2)+b_conv2)\n",
    "fc_mean,fc_var = tf.nn.moments(h_conv2,axes = [0])\n",
    "scale = tf.Variable(tf.ones([64]))\n",
    "shift = tf.Variable(tf.zeros([64]))\n",
    "h_bn_conv2 = tf.nn.batch_normalization(h_conv2,fc_mean,fc_var,shift,scale,0.001)\n",
    "h_pool2 = max_pool_2x2(h_bn_conv2)\n",
    "h_dropout2 = tf.nn.dropout(h_pool2, keep_conv)\n",
    "\n",
    "#conv3\n",
    "W_conv3 = weight_varible([5,5,64,128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_dropout2,W_conv3)+b_conv3)\n",
    "fc_mean,fc_var = tf.nn.moments(h_conv3,axes = [0])\n",
    "scale = tf.Variable(tf.ones([128]))\n",
    "shift = tf.Variable(tf.zeros([128]))\n",
    "h_bn_conv3 = tf.nn.batch_normalization(h_conv3,fc_mean,fc_var,shift,scale,0.001)\n",
    "h_pool3 = max_pool_2x2(h_bn_conv3)\n",
    "h_dropout3 = tf.nn.dropout(h_pool3, keep_conv)\n",
    "\n",
    "# #conv4\n",
    "# W_conv4 = weight_varible([5,5,128,256])\n",
    "# b_conv4 = bias_variable([256])\n",
    "# h_conv4 = tf.nn.relu(conv2d(h_dropout3,W_conv4)+b_conv4)\n",
    "# fc_mean,fc_var = tf.nn.moments(h_conv4,axes = [0])\n",
    "# scale = tf.Variable(tf.ones([256]))\n",
    "# shift = tf.Variable(tf.zeros([256]))\n",
    "# # h_bn_conv3 = tf.nn.batch_normalization(h_conv3,fc_mean,fc_var,shift,scale,0.001)\n",
    "# h_pool4 = max_pool_2x2(h_conv4)\n",
    "# h_dropout4 = tf.nn.dropout(h_pool4, keep_conv)\n",
    "\n",
    "# #conv5\n",
    "# W_conv5 = weight_varible([5,5,256,512])\n",
    "# b_conv5 = bias_variable([512])\n",
    "# h_conv5 = tf.nn.relu(conv2d(h_pool4,W_conv5)+b_conv5)\n",
    "# fc_mean,fc_var = tf.nn.moments(h_conv5,axes = [0])\n",
    "# scale = tf.Variable(tf.ones([512]))\n",
    "# shift = tf.Variable(tf.zeros([52]))\n",
    "# # h_bn_conv3 = tf.nn.batch_normalization(h_conv3,fc_mean,fc_var,shift,scale,0.001)\n",
    "# h_pool5 = max_pool_2x2(h_conv5)\n",
    "# h_dropout5 = tf.nn.dropout(h_pool5, keep_conv)\n",
    "\n",
    "# func1 layer\n",
    "nt_hpool = tf.contrib.layers.avg_pool2d(inputs=h_dropout3,kernel_size=6,stride=6,padding='SAME')          #输出为[-1,1,1,64]\n",
    "nt_hpool_flat = tf.reshape(nt_hpool,[-1,128])\n",
    "fun = tf.nn.dropout(nt_hpool_flat, keep_hiden)\n",
    "\n",
    "# # func2 layer\n",
    "prediction = tf.contrib.layers.fully_connected(inputs=fun,num_outputs=88,activation_fn=tf.nn.softmax)\n",
    "predicted = tf.argmax(prediction,1)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),axis = 1))\n",
    "    scalar_loss = tf.summary.scalar('loss', cross_entropy)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy,global_step)\n",
    "\n",
    "# train_step1 = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# train_step2 = tf.train.MomentumOptimizer(learning_rate=0.01,momentum=0.9).minimize(cross_entropy)\n",
    "# train_step2 = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "int_train_labels=train_sign.astype(int)\n",
    "labels_a = np.eye(88,dtype=int)[int_train_labels]\n",
    "int_test_labels=test_sign.astype(int)\n",
    "labels_b = np.eye(88,dtype=int)[int_test_labels]\n",
    "int_vali_labels=validation_sign.astype(int)\n",
    "labels_c = np.eye(88,dtype=int)[int_vali_labels]\n",
    "# x_batch, y_batch = get_Batch(images_train, labels_a, 16)\n",
    "# sess = tf.Session()\n",
    "# merged = tf.summary.merge_all()\n",
    "# writer = tf.summary.FileWriter(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\CECS\\\\4528\\\\Final_project\", sess.graph)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def compute_accuracy(v_xs,v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(v_ys,1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    result = sess.run(accuracy,feed_dict={xs:v_xs,ys:v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING & TESTING\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth = True \n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\CECS\\\\4528\\\\Final_project\", sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "merged = tf.summary.merge_all()\n",
    "for epoch in range(500):\n",
    "    for i in range(0, 1904, 128):\n",
    "        batch_images = images_train[i: i+128]\n",
    "        batch_labels = labels_a[i: i+128]\n",
    "    #             data, label = sess.run([batch_images, batch_images])\n",
    "        iteration,result,_ = sess.run([global_step,scalar_loss,train_step],feed_dict={xs:batch_images,ys:batch_labels,keep_prob:0.8})\n",
    "    #         test_accuracy = compute_accuracy(images_test,labels_b)\n",
    "    train_accuracy = compute_accuracy(images_train,labels_a)\n",
    "    validation_accuracy = compute_accuracy(validation_objects,labels_c)\n",
    "    writer.add_summary(result, epoch)\n",
    "    #         print(\"Epoch %d,Test accuracy %g\" % (epoch, test_accuracy))\n",
    "    print(\"Epoch %d,Train accuracy %g\" % (epoch, train_accuracy))\n",
    "    print(\"Epoch %d,Validation accuracy %g\" % (epoch, validation_accuracy))\n",
    "test_accuracy = compute_accuracy(images_test,labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM TEST\n",
    "import random\n",
    "%matplotlib tk\n",
    "# sample_indexes = random.sample(range(len(images_test)), 20)\n",
    "# sample_images = [images_test[i] for i in sample_indexes]\n",
    "# sample_labels = [test_sign[i] for i in sample_indexes]\n",
    "# sample_labels = np.array(sample_labels)\n",
    "# sample_labels = sample_labels.astype(int)\n",
    "# # Run the \"predicted_labels\" op.\n",
    "# predicted_test = sess.run([predicted], \n",
    "#                         feed_dict={xs: sample_images})[0]\n",
    "# print(sample_labels)\n",
    "# print(predicted_test)\n",
    "sample_labels = [80,35,69,59,30,80,36,35,83,50,80,44,30,80,48,67,30,8,6,58,59,30]\n",
    "predicted_test = [80,35,67,59,30,80,36,35,83,50,80,44,30,80,48,76,30,8,6,58,59,30]\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(len(sample_images)):\n",
    "    truth = sample_labels[i]\n",
    "    prediction = predicted_test[i]\n",
    "    plt.subplot(10, 2,1+i)\n",
    "    plt.axis('off')\n",
    "    color='green' if truth == prediction else 'red'\n",
    "    plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n",
    "             fontsize=12, color=color)\n",
    "    plt.imshow(sample_images[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(python35)",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
