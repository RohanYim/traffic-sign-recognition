{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_varible(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding = 'SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides=[1,2,2,1],padding = 'SAME')\n",
    "\n",
    "def identity_block(X_input, kernel_size, in_filter, out_filters, stage, block):\n",
    "        # defining name basis\n",
    "        block_name = 'res' + str(stage) + block\n",
    "        f1, f2, f3 = out_filters\n",
    "        with tf.variable_scope(block_name):\n",
    "            X_shortcut = X_input\n",
    "\n",
    "            #first\n",
    "            W_conv1 = weight_varible([1, 1, in_filter, f1])\n",
    "            X = tf.nn.conv2d(X_input, W_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            b_conv1 = bias_variable([f1])\n",
    "            X = tf.nn.relu(X+ b_conv1)\n",
    "\n",
    "            #second\n",
    "            W_conv2 = weight_varible([kernel_size, kernel_size, f1, f2])\n",
    "            X = tf.nn.conv2d(X, W_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            b_conv2 = bias_variable([f2])\n",
    "            X = tf.nn.relu(X+ b_conv2)\n",
    "\n",
    "            #third\n",
    "\n",
    "            W_conv3 = weight_varible([1, 1, f2, f3])\n",
    "            X = tf.nn.conv2d(X, W_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            b_conv3 = bias_variable([f3])\n",
    "            X = tf.nn.relu(X+ b_conv3)\n",
    "            #final step\n",
    "            add = tf.add(X, X_shortcut)\n",
    "            b_conv_fin = bias_variable([f3])\n",
    "            add_result = tf.nn.relu(add+b_conv_fin)\n",
    "\n",
    "        return add_result\n",
    "\n",
    "\n",
    "#这里定义conv_block模块，由于该模块定义时输入和输出尺度不同，故需要进行卷积操作来改变尺度，从而得以相加\n",
    "def convolutional_block( X_input, kernel_size, in_filter,\n",
    "                            out_filters, stage, block, stride=2):\n",
    "        # defining name basis\n",
    "        block_name = 'res' + str(stage) + block\n",
    "        with tf.variable_scope(block_name):\n",
    "            f1, f2, f3 = out_filters\n",
    "\n",
    "            x_shortcut = X_input\n",
    "            #first\n",
    "            W_conv1 = weight_varible([1, 1, in_filter, f1])\n",
    "            X = tf.nn.conv2d(X_input, W_conv1,strides=[1, stride, stride, 1],padding='SAME')\n",
    "            b_conv1 = bias_variable([f1])\n",
    "            X = tf.nn.relu(X + b_conv1)\n",
    "\n",
    "            #second\n",
    "            W_conv2 =weight_varible([kernel_size, kernel_size, f1, f2])\n",
    "            X = tf.nn.conv2d(X, W_conv2, strides=[1,1,1,1], padding='SAME')\n",
    "            b_conv2 = bias_variable([f2])\n",
    "            X = tf.nn.relu(X+b_conv2)\n",
    "\n",
    "            #third\n",
    "            W_conv3 = weight_varible([1,1, f2,f3])\n",
    "            X = tf.nn.conv2d(X, W_conv3, strides=[1, 1, 1,1], padding='SAME')\n",
    "            b_conv3 = bias_variable([f3])\n",
    "            X = tf.nn.relu(X+b_conv3)\n",
    "            #shortcut path\n",
    "            W_shortcut =weight_varible([1, 1, in_filter, f3])\n",
    "            x_shortcut = tf.nn.conv2d(x_shortcut, W_shortcut, strides=[1, stride, stride, 1], padding='VALID')\n",
    "\n",
    "            #final\n",
    "            add = tf.add(x_shortcut, X)\n",
    "            #建立最后融合的权重\n",
    "            b_conv_fin = bias_variable([f3])\n",
    "            add_result = tf.nn.relu(add+ b_conv_fin)\n",
    "\n",
    "\n",
    "        return add_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=tf.placeholder(tf.float32,[None,24,24,3])\n",
    "ys=tf.placeholder(tf.float32,[None,88])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "keep_hide = tf.placeholder(tf.float32)\n",
    "# lr1 = tf.Variable(0.0001, dtype=tf.float32)  \n",
    "global_step = tf.Variable(0, dtype=tf.int32)\n",
    "# lr2 = tf.Variable(0.00001, dtype=tf.float32)\n",
    "# lr3 = tf.Variable(0.000005, dtype=tf.float32)  \n",
    "lr = 0.0001\n",
    "# learing_rate = tf.train.exponential_decay(0.001,120,10,0.96,staircase=False)\n",
    "x_image = tf.reshape(xs,[-1,24,24,3])\n",
    "keep_conv = 0.8\n",
    "keep_hiden = 0.5\n",
    "\n",
    "# #conv0\n",
    "# W_conv0 = weight_varible([5,5,3,16])\n",
    "# b_conv0 = bias_variable([16])\n",
    "# h_conv0 = tf.nn.relu(conv2d(x_image,W_conv0)+b_conv0)\n",
    "# fc_mean,fc_var = tf.nn.moments(h_conv0,axes = [0])\n",
    "# scale = tf.Variable(tf.ones([16]))\n",
    "# shift = tf.Variable(tf.zeros([16]))\n",
    "# # h_LRN_conv1 = tf.nn.local_response_normalization(h_conv1,1,0,1,1)\n",
    "# # h_bn_conv1 = tf.nn.batch_normalization(h_conv1,fc_mean,fc_var,shift,scale,0.001)\n",
    "# h_pool0 = max_pool_2x2(h_conv0)\n",
    "# h_dropout0 = tf.nn.dropout(h_pool0, keep_conv)\n",
    "\n",
    "# conv1 layer\n",
    "W_conv1 = weight_varible([5,5,3,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "fc_mean,fc_var = tf.nn.moments(h_conv1,axes = [0])\n",
    "scale = tf.Variable(tf.ones([32]))\n",
    "shift = tf.Variable(tf.zeros([32]))\n",
    "# h_LRN_conv1 = tf.nn.local_response_normalization(h_conv1,1,0,1,1)\n",
    "h_bn_conv1 = tf.nn.batch_normalization(h_conv1,fc_mean,fc_var,shift,scale,0.001)\n",
    "h_pool1 = max_pool_2x2(h_bn_conv1)\n",
    "h_dropout1 = tf.nn.dropout(h_pool1, keep_conv)\n",
    "\n",
    "#stage 2\n",
    "x = convolutional_block(X_input=h_pool1, kernel_size=3, in_filter=32,  out_filters=[32, 32, 128], stage=2, block='a', stride=1)\n",
    "#上述conv_block操作后，尺寸变为14x14x256\n",
    "x = identity_block(x, 3, 128, [32, 32, 128], stage=2, block='b' )\n",
    "x = identity_block(x, 3, 128, [32, 32, 128], stage=2, block='c')\n",
    "#上述操作后张量尺寸变成14x14x256\n",
    "x = tf.nn.max_pool(x, [1, 2, 2, 1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "# #stage 3\n",
    "# x = convolutional_block(x, 3, 64, [64,64,128], 3, 'a')\n",
    "# x = identity_block(x, 3, 128, [64,64,128], 3, 'b')\n",
    "# x = identity_block(x, 3, 128, [64,64,128], 3, 'c')\n",
    "# x = identity_block(x, 3, 128, [64,64,128], 3, 'd')\n",
    "\n",
    "# #stage 4\n",
    "# x = self.convolutional_block(x, 3, 512, [256, 256, 1024], 4, 'a')\n",
    "# x = self.identity_block(x, 3, 1024, [256, 256, 1024], 4, 'b')\n",
    "# x = self.identity_block(x, 3, 1024, [256, 256, 1024], 4, 'c')\n",
    "# x = self.identity_block(x, 3, 1024, [256, 256, 1024], 4, 'd')\n",
    "# x = self.identity_block (x, 3, 1024, [256, 256, 1024], 4, 'e')\n",
    "# x = self.identity_block(x, 3, 1024, [256, 256, 1024], 4, 'f')\n",
    "\n",
    "# #stage 5\n",
    "# x = self.convolutional_block(x, 3, 1024, [512, 512, 2048], 5, 'a')\n",
    "# x = self.identity_block(x, 3, 2048, [512, 512, 2048], 5, 'b')\n",
    "# x = self.identity_block(x, 3, 2048, [512, 512, 2048], 5, 'c')\n",
    "# func1 layer\n",
    "nt_hpool = tf.contrib.layers.avg_pool2d(inputs=x,kernel_size=6,stride=6,padding='SAME')          #输出为[-1,1,1,64]\n",
    "nt_hpool_flat = tf.reshape(nt_hpool,[-1,128])\n",
    "# fun = tf.nn.dropout(nt_hpool_flat, keep_hiden)\n",
    "# logits = tf.layers.dense(nt_hpool_flat, units=88, activation=tf.nn.softmax)\n",
    "# # func2 layer\n",
    "prediction = tf.contrib.layers.fully_connected(inputs=nt_hpool_flat,num_outputs=88,activation_fn=tf.nn.softmax)\n",
    "predicted = tf.argmax(logits,1)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),axis = 1))\n",
    "    scalar_loss = tf.summary.scalar('loss', cross_entropy)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy,global_step)\n",
    "\n",
    "# train_step1 = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# train_step2 = tf.train.MomentumOptimizer(learning_rate=0.01,momentum=0.9).minimize(cross_entropy)\n",
    "# train_step2 = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "int_train_labels=train_sign.astype(int)\n",
    "labels_a = np.eye(88,dtype=int)[int_train_labels]\n",
    "int_test_labels=test_sign.astype(int)\n",
    "labels_b = np.eye(88,dtype=int)[int_test_labels]\n",
    "# x_batch, y_batch = get_Batch(images_train, labels_a, 16)\n",
    "# sess = tf.Session()\n",
    "# merged = tf.summary.merge_all()\n",
    "# writer = tf.summary.FileWriter(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\CECS\\\\4528\\\\Final_project\", sess.graph)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def compute_accuracy(v_xs,v_ys):\n",
    "    global logits\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "    result = sess.run(accuracy,feed_dict={xs:v_xs,ys:v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(ys,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "## steps\n",
    "# for i in range(11):\n",
    "#         int_train_labels=labels_train.astype(int)\n",
    "#         labels_a = np.eye(11,dtype=int)[int_train_labels]\n",
    "#         int_test_labels=labels_test.astype(int)\n",
    "#         labels_b = np.eye(11,dtype=int)[int_test_labels]\n",
    "#         batch_xs,batch_ys = get_batches(images_train,labels_a,24,24,16,20)\n",
    "#         sess.run(train_step1,feed_dict={xs:batch_xs,ys:batch_ys,keep_prob:0.8})\n",
    "# #         if i<=500:\n",
    "# #             sess.run(train_step1,feed_dict={xs:images_train,ys:labels_a,keep_prob:0.8})\n",
    "# #         elif i<=2000:\n",
    "# #             sess.run(train_step2,feed_dict={xs:images_train,ys:labels_a,keep_prob:0.8})\n",
    "# #         else:\n",
    "# #             sess.run(train_step3,feed_dict={xs:images_train,ys:labels_a,keep_prob:0.8})\n",
    "#         if i%2==0:\n",
    "#             train_accuracy = compute_accuracy(images_test,labels_b)\n",
    "#             print('Step {0} tranining accuracy {1}'.format(i,train_accuracy))\n",
    "\n",
    "# Epoches\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth = True \n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\CECS\\\\4528\\\\Final_project\", sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "merged = tf.summary.merge_all()\n",
    "for epoch in range(100):\n",
    "    for i in range(0, 1904, 16):\n",
    "        batch_images = images_train[i: i+16]\n",
    "        batch_labels = labels_a[i: i+16]\n",
    "    #             data, label = sess.run([batch_images, batch_images])\n",
    "        iteration,result,_ = sess.run([global_step,scalar_loss,train_step],feed_dict={xs:batch_images,ys:batch_labels,keep_prob:0.8})\n",
    "    #         test_accuracy = compute_accuracy(images_test,labels_b)\n",
    "    train_accuracy = compute_accuracy(images_train,labels_a)\n",
    "    writer.add_summary(result, epoch)\n",
    "    #         print(\"Epoch %d,Test accuracy %g\" % (epoch, test_accuracy))\n",
    "    print(\"Epoch %d,Train accuracy %g\" % (epoch, train_accuracy))\n",
    "test_accuracy = compute_accuracy(images_test,labels_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(python35)",
   "language": "python",
   "name": "python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
